import json
import math
import os
import threading
import time
from typing import Optional

import cv2
import numpy as np
import requests
import torch
import yaml


from ultralytics import YOLO

from src.domain.ActionState import action_state
from src.domain.StreamCamState import StreamCamState
from src.domain.StreamState import StreamState
from src.mq.MQTTPublisher import create_default_publisher

try:
    publisher = create_default_publisher(brokers=["192.168.0.102"], topic="jetauto/cmd")

except Exception as e:
    print(e)

    # human_publish = create_default_publisher(brokers=["192.168.0.101"], topic="jetauto/cmd")


# MODEL_PATH = r"D:\programs\python_projects\quest_robots\yolo_model\ball_120.pt"
MODEL_PATH = r"D:\programs\python_projects\quest_robots\yolo_model\yolov8s-worldv2.pt"

# åŠ è½½æ¨¡å‹
if not os.path.isfile(MODEL_PATH):
    raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶: {MODEL_PATH}")
print(f"âœ… åŠ è½½æ¨¡å‹: {MODEL_PATH}")

# 1. é€‰ device
DEVICE = "cuda:0" if torch.cuda.is_available() else "cpu"
yolo_model = YOLO(MODEL_PATH)


yolo_model.to(DEVICE)   # ğŸŒŸ ä¿è¯æ¨¡å‹å’Œåé¢çš„ device ä¸€è‡´





# ---------- ç›¸æœºå†…å‚ K ----------
K = np.array([
    [473.4506985179141, 0.0, 323.5512181265506],
    [0.0, 474.2169451085363, 238.6016133237558],
    [0.0, 0.0, 1.0]
], dtype=np.float32)

# ---------- ç•¸å˜ç³»æ•° D ----------
dist = np.array([
    -0.02749631166333748,
    -0.0005351606400560652,
    -0.005899822232353089,
    -0.003423120278803321,
    -0.03559287109655516
], dtype=np.float32)

# å›¾åƒå°ºå¯¸ï¼ˆå®½, é«˜ï¼‰
img_size = (640, 480)

# ---------- ä½¿ç”¨ projection_matrix é‡Œçš„æ–°å†…å‚ ----------
K_new = np.array([
    [465.8162841796875, 0.0, 321.2985662909559],
    [0.0, 470.4940490722656, 236.298464380614],
    [0.0, 0.0, 1.0]
], dtype=np.float32)

# ç”Ÿæˆå»ç•¸å˜æ˜ å°„ï¼ˆåªåšä¸€æ¬¡ï¼‰
map1, map2 = cv2.initUndistortRectifyMap(
    K, dist,
    R=None,  # rectification_matrix æ˜¯å•ä½é˜µï¼Œç”¨ None å³å¯
    newCameraMatrix=K_new,
    size=img_size,
    m1type=cv2.CV_16SC2
)

FPS = 20
JPEG_QUALITY = 80

# MJPEG æ‹‰æµåœ°å€
Push_URL = "http://192.168.0.100:8000/push/detect_cam"





# HSV é¢œè‰²èŒƒå›´ï¼ˆç¤ºä¾‹ï¼Œæ ¹æ®å®é™…ç¯å¢ƒå†è°ƒï¼‰
COLOR_RANGES = {
    "red": [
        (np.array([0, 120, 70]), np.array([10, 255, 255])),
        (np.array([170, 120, 70]), np.array([180, 255, 255])),
    ],
    "blue": [
        (np.array([100, 120, 70]), np.array([130, 255, 255])),
    ],
    "green": [
        (np.array([40, 70, 70]), np.array([80, 255, 255])),
    ],
    "yellow": [
        (np.array([20, 120, 120]), np.array([35, 255, 255])),
    ],
    "white": [
        # ç™½è‰²: ä½é¥±å’Œåº¦ + é«˜äº®åº¦
        (np.array([0, 0, 200]), np.array([180, 40, 255])),
    ],
}

def load_bbox_from_yaml(path, margin=None):
    """
    ä»åŒ…å«ä»¥ä¸‹ç»“æ„çš„ YAML æ–‡ä»¶è¯»å– bboxï¼š
      roi:
        x_min: ...
        x_max: ...
        y_min: ...
        y_max: ...

    è‹¥æ–‡ä»¶é‡Œçš„ bbox æ˜¯â€œæ‰©å¤§çš„â€ï¼ˆä¾‹å¦‚ expand_bbox(margin=10) åä¿å­˜çš„ï¼‰ï¼Œ
    å¯ä»¥ä¼ å…¥ margin è¿›è¡Œåæ¨è¿˜åŸï¼š
        margin=10  â†’ å®½é«˜å„å‡ 20
    """
    if not os.path.exists(path):
        print(f"[ERROR] ROI YAML not found: {path}")
        return None

    with open(path, "r", encoding="utf-8") as f:
        data = yaml.safe_load(f)

    if "roi" not in data:
        print("[ERROR] YAML missing 'roi' field")
        return None

    r = data["roi"]
    x_min = int(r["x_min"])
    x_max = int(r["x_max"])
    y_min = int(r["y_min"])
    y_max = int(r["y_max"])

    x = x_min
    y = y_min
    w = x_max - x_min
    h = y_max - y_min

    # å¦‚æœ YAML é‡Œä¿å­˜çš„æ˜¯â€œæ‰©å¤§çš„ bboxâ€ï¼Œè¿™é‡Œå¯åæ¨å›æ­£å¸¸ bbox
    if margin is not None and margin > 0:
        x += margin
        y += margin
        w -= margin * 2
        h -= margin * 2

    return (x, y, w, h)

def draw_bbox(frame, bbox, color=(0, 255, 0), label=None, thickness=2):
    """
    åœ¨ frame ä¸Šç»˜åˆ¶ä¸€ä¸ªçŸ©å½¢æ¡†ï¼ˆbboxï¼‰ã€‚

    å‚æ•°:
        frame : np.ndarray    - BGR å›¾åƒ
        bbox  : (x, y, w, h)  - å¤–æ¥æ¡†
        color : (B, G, R)     - é¢œè‰²
        label : str æˆ– None   - åœ¨æ¡†ä¸Šæ–¹å†™å­—
        thickness: int        - çº¿å®½
    """
    if bbox is None:
        return frame

    x, y, w, h = bbox

    # ç”»çŸ©å½¢
    cv2.rectangle(frame, (x, y), (x + w, y + h), color, thickness)

    # ç”»æ–‡å­—
    if label is not None:
        cv2.putText(
            frame, label,
            (x, y - 10),
            cv2.FONT_HERSHEY_SIMPLEX,
            0.6, color, 2
        )

    return frame




# ===============================pull  and push streaming ==================================



"""
    ä» FastAPI çš„ /stream/{stream_id} (MJPEG) æ‹‰æµï¼Œ
    è§£ç æ¯ä¸€å¸§ï¼Œå¹¶å†™å…¥ StreamStateã€‚
    """

def get_opencv_frame(stream: StreamState):
    """
    ä» FastAPI çš„ stream.latest_frame (JPEG bytes) è§£ç æˆ BGR å›¾åƒ
    """
    with stream._lock:
        if stream.latest_frame is None:
            return None

        jpg_bytes = stream.latest_frame

    # JPEG â†’ numpy array â†’ BGR
    nparr = np.frombuffer(jpg_bytes, np.uint8)
    frame = cv2.imdecode(nparr, cv2.IMREAD_COLOR)

    return frame




# convert opencv frame to jpeg
def frame_to_jpeg(frame: Optional[np.ndarray], quality: int = 80) -> Optional[bytes]:
    """
    å°† OpenCV BGR å›¾åƒè½¬æˆ FastAPI StreamState.latest_frame å¯ç›´æ¥ä½¿ç”¨çš„ JPEG bytesã€‚

    è¾“å…¥:
        frame: np.ndarray æˆ– None
    è¾“å‡º:
        Optional[bytes] â€”â€” ä¸ FastAPI æ¨æµç»“æ„ä¿æŒä¸€è‡´
    """
    if frame is None:
        return None

    ok, buf = cv2.imencode(".jpg", frame,
                           [int(cv2.IMWRITE_JPEG_QUALITY), quality])
    if not ok:
        return None

    return buf.tobytes()

def push_loop(state, url, name):

    session = requests.Session()
    frame_interval = 1.0 / FPS
    last_time = 0

    print(f"ğŸ“¡ æ¨æµçº¿ç¨‹å¯åŠ¨ [{name}] â†’ {url}")

    while True:
        now = time.time()
        if now - last_time < frame_interval:
            time.sleep(0.001)
            continue
        last_time = now

        frame = state.get_frame_copy()
        # ret, frame = cap.read()
        if frame is None:
            # print(f"âš ï¸ {name} è¯»å–å¤±è´¥")
            time.sleep(0.05)
            continue

        ok, buf = cv2.imencode(".jpg", frame,
                               [int(cv2.IMWRITE_JPEG_QUALITY), JPEG_QUALITY])
        if not ok:
            print(f"âš ï¸ {name} JPEG ç¼–ç å¤±è´¥")
            continue

        try:
            session.post(
                url,
                data=buf.tobytes(),
                headers={"Content-Type": "image/jpeg"},
                timeout=0.5
            )
        except Exception as e:
            print(f"âš ï¸ {name} æ¨æµå¼‚å¸¸: {e}")
            time.sleep(0.2)


# stable check =======================================================================
def check_stable(dx, dy, tolerance=5, threshold=100):
    """
    åˆ¤æ–­ dxã€dy æ˜¯å¦è¿ç»­ç¨³å®š threshold æ¬¡ã€‚
    è¿”å›å€¼ï¼š
        - None ï¼šæœªç¨³å®š
        - (dx, dy) ï¼šç¨³å®šåçš„æœ€ç»ˆå€¼
    """

    if not hasattr(check_stable, "prev_dx"):
        check_stable.prev_dx = None
        check_stable.prev_dy = None
        check_stable.counter = 0

    if check_stable.prev_dx is not None:
        dx_stable = abs(dx - check_stable.prev_dx) <= tolerance
        dy_stable = abs(dy - check_stable.prev_dy) <= tolerance

        if dx_stable and dy_stable:
            check_stable.counter += 1
        else:
            check_stable.counter = 0
    else:
        check_stable.counter = 0

    check_stable.prev_dx = dx
    check_stable.prev_dy = dy

    if check_stable.counter >= threshold:
        return (dx, dy)

    return None

# ==================================images process area=======================================================
def detect_color(state = None,frame_in=None):
    # frame_in = None, state = None
    """
    ä» StreamState ä¸­å–æœ€æ–°çš„ä¸€å¸§å›¾åƒï¼Œåšå»ç•¸å˜ + é¢œè‰²æ£€æµ‹ + åœ†å½¢å½¢æ€å­¦è¿‡æ»¤ã€‚
    è¿”å›: (best_color_name, best_area, best_bbox, frame_undistorted)
      - best_color_name: æœ€ä¼˜é¢œè‰²åç§°(str) æˆ– None
      - best_area: æœ€å¤§è½®å»“é¢ç§¯(float)ï¼ˆå·²ç»é€šè¿‡åœ†å½¢åº¦è¿‡æ»¤ï¼‰
      - best_bbox: (x, y, w, h) æˆ– None
      - frame_undistorted: å»ç•¸å˜åçš„å›¾åƒ
    """
    frame = frame_in
    if state is not None:
        frame = state.get_frame_copy()

    if frame is None:
        return None, 0.0, None, None

    # 1. å»ç•¸å˜
    frame = cv2.remap(frame, map1, map2, cv2.INTER_LINEAR)

    # 2. BGR -> HSV
    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)

    best_color = None
    best_score = 0.0  # ç”¨ area * circularity ä½œä¸ºç»¼åˆè¯„åˆ†
    best_area = 0.0
    best_bbox = None  # æœ€å¤§ç›®æ ‡çš„å¤–æ¥çŸ©å½¢

    # å½¢æ€å­¦å†…æ ¸ï¼ˆå¯æŒ‰éœ€æ±‚è°ƒå¤§/è°ƒå°ï¼‰
    kernel = np.ones((5, 5), np.uint8)

    # é¢ç§¯å’Œåœ†å½¢åº¦é˜ˆå€¼ï¼Œåé¢å¯ä»¥æ ¹æ®å®é™…ç”»é¢æ…¢æ…¢è°ƒ
    MIN_AREA = 330.0  # åƒç´ é¢ç§¯ï¼Œæ¯”è¿™ä¸ªå°çš„ç›´æ¥å¿½ç•¥
    MIN_CIRCULARITY = 0.6  # 0~1ï¼Œè¶Šæ¥è¿‘1è¶Šåœ†ï¼Œ0.6~0.8 æ¯”è¾ƒå¸¸ç”¨

    for color_name, ranges in COLOR_RANGES.items():
        # 2.1 åˆå¹¶è¯¥é¢œè‰²çš„æ‰€æœ‰ HSV åŒºé—´
        mask_total = None
        for lower, upper in ranges:
            mask = cv2.inRange(hsv, lower, upper)
            if mask_total is None:
                mask_total = mask
            else:
                mask_total = cv2.bitwise_or(mask_total, mask)

        if mask_total is None:
            continue

        # 2.2 å½¢æ€å­¦æ“ä½œï¼šå»å°å™ªç‚¹ + å¡«å°æ´
        mask_total = cv2.morphologyEx(mask_total, cv2.MORPH_OPEN, kernel)
        mask_total = cv2.morphologyEx(mask_total, cv2.MORPH_CLOSE, kernel)

        # 2.3 æ‰¾è½®å»“
        contours, _ = cv2.findContours(mask_total, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

        for c in contours:
            area = cv2.contourArea(c)

            if area < MIN_AREA:
                continue  # é¢ç§¯å¤ªå°å½“ä½œå™ªå£°
            # print(f"=========ares:{area}")
            # è®¡ç®—åœ†å½¢åº¦ circularity = 4Ï€A / P^2
            perimeter = cv2.arcLength(c, True)
            if perimeter <= 0:
                continue
            circularity = 4.0 * np.pi * area / (perimeter * perimeter)

            if circularity < MIN_CIRCULARITY:
                # è¿‡æ»¤æ‰ç»†é•¿ã€æ‰é•¿ã€ä¸è§„åˆ™çš„åŒºåŸŸ
                continue

            # é€šè¿‡äº†é¢ç§¯ + åœ†å½¢åº¦æ£€æŸ¥ï¼Œè®¤ä¸ºæ˜¯å€™é€‰â€œçƒâ€
            x, y, w, h = cv2.boundingRect(c)

            # ç»¼åˆè¯„åˆ†ï¼šé¢ç§¯ * åœ†å½¢åº¦ï¼ˆå¤§ä¸”åœ†ï¼‰
            score = area * circularity

            if score > best_score:
                best_score = score
                best_area = area
                best_color = color_name
                best_bbox = (x, y, w, h)

    return best_color, best_area, best_bbox, frame




def bbox_center_xywh(bbox):
    x, y, w, h = bbox
    cx = int(x + w / 2)
    cy = int(y + h / 2)
    return cx, cy

# ç®—å‡ºè·ç¦»åå·®
def draw_right_triangle_and_offsets(frame, bbox1, bbox2,
                                    color_line=(0, 255, 255),
                                    color_point1=(0, 0, 255),
                                    color_point2=(0, 255, 0)):
    """
    åœ¨ frame ä¸Šï¼š
      - ç”»å‡º bbox1ã€bbox2 çš„ä¸­å¿ƒç‚¹
      - ç”¨è¿™ä¸¤ä¸ªç‚¹ç”»ä¸€ä¸ªç›´è§’ä¸‰è§’å½¢ï¼ˆæ°´å¹³ + å‚ç›´ + æ–œè¾¹ï¼‰
      - åˆ†åˆ«æ˜¾ç¤º xã€y æ–¹å‘çš„åƒç´ è·ç¦»ï¼ˆå«æ­£è´Ÿï¼‰

    è¿”å›: frame, dx, dy, dist
      dx, dy: c2 ç›¸å¯¹ c1 çš„æ°´å¹³/å‚ç›´åç§»ï¼ˆåƒç´ ï¼Œå¯ä¸ºè´Ÿï¼‰
      dist:   ä¸¤ç‚¹æ¬§æ°è·ç¦»ï¼ˆåƒç´ ï¼‰
    """
    if bbox1 is None or bbox2 is None:
        return frame, None, None, None

    c1 = bbox_center_xywh(bbox1)
    c2 = bbox_center_xywh(bbox2)

    x1, y1 = c1
    x2, y2 = c2

    dx = x2 - x1  # >0 è¯´æ˜ c2 åœ¨ c1 å³ä¾§
    dy = y2 - y1  # >0 è¯´æ˜ c2 åœ¨ c1 ä¸‹æ–¹
    dist = math.hypot(dx, dy)

    return frame, dx, dy, dist


"""
first process dx   then process dy
"""


def action_proc_new(dx, dy):
    # if -25 < dy < 25 and -55 < dy < 55:
    def dx_proc(dx):
        x_linear = 0.1
        x_direction = 1
        if dx < 0:
            x_direction = -1

        x_p_distance = abs(dx)

        #  å¦‚æœåƒç´ é•¿åº¦å¤§äº100 é‚£ä¹ˆå°±æœ€å¤šèµ°2s æ§åˆ¶
        if x_p_distance > 100:
            x_action_duration = x_p_distance // 100
            x_action_duration = 2 if x_action_duration >= 2 else 1
        else:
            if x_p_distance >= 50:
                x_linear = 0.03
            else:
                x_linear = 0.02
            x_action_duration = 1

        data = [0, x_linear * x_direction, x_action_duration]

        payload = {"type": 'move', "data": data}
        publisher.publish(json.dumps(payload))

    def dy_proc(dy):
        y_linear = 0.1
        y_direction = 1
        if dy < 0:
            y_direction = -1

        y_p_distance = abs(dy)

        #  å¦‚æœåƒç´ é•¿åº¦å¤§äº100 é‚£ä¹ˆå°±æœ€å¤šèµ°2s æ§åˆ¶
        if y_p_distance > 100:
            y_action_duration = y_p_distance // 100
            y_action_duration = 2 if y_action_duration >= 2 else 1
        else:
            if y_p_distance >= 50:
                y_linear = 0.03
            else:
                y_linear = 0.02

            y_action_duration = 1

        data = [y_linear * y_direction, 0, y_action_duration]

        payload = {"type": 'move', "data": data}
        publisher.publish(json.dumps(payload))

    # å¦‚æœåœ¨èŒƒå›´å†…å°±ç›´æ¥æ¡çƒ

    if -40 <= dx <=40 and -25 <= dy <= 30:
        cmd_type = 'cmd'
        payload = {"type": cmd_type, "data": "pick_place_ball_big_craw"}
        publisher.publish(json.dumps(payload))
        time.sleep(1.5)
        action_state.set_picking(False)
        return

    else:
        # if pick_flag: return
        if abs(dx)>40:
            dx_proc(dx)
            time.sleep(1.5)
        else:
            if dy < -25 or dy > 30:
                dy_proc(dy)
                time.sleep(1.5)


IMG_SIZE = 640
CONF_THRES = 0.3        # ç½®ä¿¡åº¦é˜ˆå€¼(0~1)
DEVICE = "0"

# ====================================æ£€æµ‹çƒä½“==========================================
def run_detect_script(pull_state:StreamState,is_yolo:bool):
    roi_path = r"src/cfg/pick_roi.yaml"
    roi_bbox = load_bbox_from_yaml(roi_path)

    push_state = StreamCamState()
    t2 = threading.Thread(target=push_loop, args=(push_state, Push_URL, 'detect_cam'), daemon=True)
    t2.start()
    def detect_by_yolo(frame):
        detect_class = action_state.get_detect_class()
        if detect_class:
            yolo_model.set_classes(detect_class)
        results = yolo_model.predict(
            source=frame,
            imgsz=IMG_SIZE,
            conf=CONF_THRES,
            device=DEVICE,
            save=False,  # â— ä¸ä¿å­˜
            save_txt=False,  # â— ä¸ä¿å­˜ txt
            verbose=False,
            show=False,

            max_det=20,  # âœ… é™åˆ¶æ¯å¸§æœ€å¤šæ£€æµ‹å¤šå°‘ä¸ªç›®æ ‡ï¼ˆé»˜è®¤ 300ï¼‰ï¼Œå‡å°‘ NMS å¼€é”€
            agnostic_nms=False,  # âœ… ç±»åˆ«æ— å…³ NMS ä¸€èˆ¬ä¸éœ€è¦ï¼Œå…³æ‰ç•¥å¿«ä¸€ç‚¹
            vid_stride=1,
        )

        return  results

        # if cv2.waitKey(1) & 0xFF == ord('q'):
        #     break

    def detect_by_color():
        color, area, bbox, frame = detect_color(frame_in=in_frame)
        if frame is None:
            time.sleep(0.05)
            return

        # åœ¨å›¾åƒä¸Šç”»æ£€æµ‹åˆ°çš„æœ€å¤§é¢œè‰²å—
        if bbox is not None and color is not None:



            # é¢œè‰²æ¡†ï¼ˆé»„è‰²ï¼‰
            draw_bbox(frame, bbox, color=(0, 255, 255), label="BALL")

            frame, dx, dy, dist = draw_right_triangle_and_offsets(frame, bbox, roi_bbox)

            # å‡†å¤‡è¦æ˜¾ç¤ºçš„æ–‡å­—
            text1 = f"dx: {dx:.1f} px"
            text2 = f"dy:   {dy:.1f} px"
            text_tips = f"The ball has been detected"
            # å­—ä½“ & é¢œè‰²è®¾ç½®
            font = cv2.FONT_HERSHEY_SIMPLEX
            color = (0, 255, 0)  # ç»¿è‰²
            thickness = 2
            cv2.putText(frame, text_tips, (20, 40), font, 0.8, color, thickness)
            # åœ¨ç”»é¢å·¦ä¸Šè§’ä¾æ¬¡æ˜¾ç¤ºä¸‰è¡Œ
            # cv2.putText(frame, text1, (20, 40), font, 0.8, color, thickness)
            # cv2.putText(frame, text2, (20, 70), font, 0.8, color, thickness)

            # 4. è°ƒç”¨ç¨³å®šåˆ¤æ–­
            stable = check_stable(dx, dy)

            if stable is not None:
                final_dx, final_dy = stable
                # print("ğŸ“Œ dx dy å·²è¿ç»­ç¨³å®š 100 æ¬¡ï¼š", final_dx, final_dy)
                if action_state.is_picking():
                    action_proc_new(final_dx, final_dy)

            return  frame

    try:
        while True:
            # é¢œè‰²æ£€æµ‹ï¼ˆå«å»ç•¸å˜ï¼‰
            in_frame = get_opencv_frame(pull_state)
            if is_yolo:
                # action_state.set_start_detect(True)
                is_detect = action_state.get_start_detect()
                if is_detect:
                    yolo_start = time.perf_counter()
                    res = detect_by_yolo(in_frame)
                    yolo_time = (time.perf_counter() - yolo_start) * 1000  # æ¯«ç§’
                    print(f"[YOLO] Inference time: {yolo_time:.2f} ms")
                    # YOLO è¿”å›ä¸€ä¸ª listï¼Œæ‰€ä»¥å–ç¬¬ä¸€é¡¹
                    annotated = res[0].plot(conf=False)  # å¸¦æ¡†çš„ BGR å›¾åƒ
                    push_state.latest_frame = annotated
                    # if class_len == 1:
                    #     count = len(res[0].boxes)
                    #     print("æ£€æµ‹åˆ°ç›®æ ‡æ•°é‡:", count)
                    #     voice_text = f"I detect {count} {detectClasses[0]}"


                    cv2.imshow("YOLO Stream", annotated)
                    if cv2.waitKey(1) & 0xFF == ord('q'):
                        break
                else: push_state.latest_frame = in_frame

            else:
                out_frame = detect_by_color()

                # push_frame = frame_to_jpeg(frame)
                push_state.latest_frame = out_frame
                # æ˜¾ç¤ºç”»é¢
                # cv2.imshow("Object Detection", out_frame)
                # if cv2.waitKey(1) & 0xFF == ord('q'):
                #     break

    except KeyboardInterrupt:
        print("\n[Main] Exit by user.")
    finally:

        cv2.destroyAllWindows()





